{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae2ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guardrail_dataset import IS_Dataset\n",
    "from guardrail_dataset_test import IS_Dataset_test\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils import data\n",
    "from torchvision import transforms, utils\n",
    "import torch.nn.utils.prune as prune\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import random\n",
    "import torch.optim as opt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from thop import profile\n",
    "\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "import imgviz\n",
    "\n",
    "import re\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b6c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images(root_path): \n",
    "  \"\"\" \n",
    "    從目錄資料夾中讀出所有檔名 \n",
    "    Args : directory path\n",
    "    Return : list with file path\n",
    "  \"\"\"\n",
    "\n",
    "  for root, dirs, files in os.walk(root_path):\n",
    "    print(\"path：\", root)\n",
    "    print(\"directory：\", dirs)\n",
    "    print(\"file：\", files)\n",
    "    \n",
    "  org_imgs = [(root + \"/\" + file_name) for file_name in tqdm(files)]\n",
    "  org_imgs.sort()\n",
    "  print(\"Read \",len(org_imgs), \" files\")\n",
    "  return org_imgs\n",
    "\n",
    "def make_cm2lbl():\n",
    "\n",
    "  colormap = [[0,0,0], [0,0,128], [0,128,128], [128,0,0], [0,128,0]] # background, grider, net, lanyard, guardrail \n",
    "\n",
    "  cm2lbl = np.zeros(256**3) # for each pixel 0~255 , channel = 3(RGB)\n",
    "  for i,cm in enumerate(colormap):\n",
    "      cm2lbl[(cm[0]*256+cm[1])*256+cm[2]] = i\n",
    "      # print(i)\n",
    "      # print((cm[0]*256+cm[1])*256+cm[2]\n",
    "  return cm2lbl\n",
    "\n",
    "\n",
    "def image2label(img, cm2lbl):\n",
    "  data = np.array(img, dtype='int32')\n",
    "  idx = (data[:, :, 0] * 256 + data[:, :, 1]) * 256 + data[:, :, 2]\n",
    "  # print(np.unique(idx))\n",
    "  # print(cm2lbl[idx])\n",
    "  result = np.array(cm2lbl[idx], dtype='int64')  \n",
    "  return result[:,:,None]\n",
    "\n",
    "# helper function for data visualization\n",
    "def visualize(**images):\n",
    "  \"\"\"PLot images in one row.\"\"\"\n",
    "  n = len(images)\n",
    "  plt.figure(figsize=(16, 5))\n",
    "  for i, (name, image) in enumerate(images.items()):\n",
    "    plt.subplot(1, n, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(' '.join(name.split('_')).title())\n",
    "    plt.imshow(image)\n",
    "  plt.show()\n",
    "\n",
    "def print_network(net):\n",
    "  num_params = 0\n",
    "  for param in net.parameters():\n",
    "    num_params += param.numel()\n",
    "  print(net)\n",
    "  print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71c9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_viz(pr_mask_gray_uni_list, pr_mask_gray, image_vis, classes):\n",
    "  idx = range(len(pr_mask_gray_uni_list))\n",
    "\n",
    "  vis_dict = dict(zip(pr_mask_gray_uni_list, idx))\n",
    "\n",
    "  pr_mask_viz = pr_mask_gray\n",
    "    \n",
    "  for i in range(pr_mask_viz.shape[0]):\n",
    "    for j in range(pr_mask_viz.shape[1]):\n",
    "      pr_mask_viz[i][j] = vis_dict[pr_mask_viz[i][j]]\n",
    "\n",
    "  pr_mask_viz = pr_mask_viz.astype(np.int32)\n",
    "  print('pr_mask_viz shape: ',pr_mask_viz.shape)\n",
    "  lbl_viz = imgviz.label2rgb(\n",
    "      label=pr_mask_viz, image=imgviz.asgray(image_vis), label_names=classes, loc=\"rb\"\n",
    "  )\n",
    "  \n",
    "  return lbl_viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2dc2de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ratioFunction(num1, num2):\n",
    "    num1 = int(num1) # Now we are good\n",
    "    num2 = int(num2) # Good, good\n",
    "    if num1 > num2:\n",
    "        ratio12 = num2/num1\n",
    "    else:\n",
    "        ratio12 = num1/num2\n",
    "#     print('The ratio of', num1, 'and', num2,'is', ratio12 + '.')\n",
    "    return ratio12\n",
    "ratioFunction(288, 216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79f3474f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IS_Dataset_test(Dataset):\n",
    "  CLASSES = ['background', 'girder', 'net', 'lanyard', 'guardrail']\n",
    "  def __init__(self, data_list, label_list, classes, cm2lbl, transform=None, transform_toTensor=None, resize_size=None):\n",
    "\n",
    "    self.data_list = data_list\n",
    "    self.label_list = label_list\n",
    "    self.transform = transform\n",
    "    self.transform_toTensor = transform_toTensor\n",
    "    self.cm2lbl = cm2lbl\n",
    "    self.resize_size = resize_size\n",
    "    # convert str names to class values on masks\n",
    "    self.class_values = [self.CLASSES.index(cls.lower()) for cls in classes]\n",
    "    print('Read ' + str(len(self.data_list)) + ' images')\n",
    "    \n",
    "  def __getitem__(self, index):\n",
    "    # read data\n",
    "    image = cv2.imread(self.data_list[index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     image = cv2.resize(image, self.resize_size, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "    mask = cv2.imread(self.label_list[index])\n",
    "#     mask = cv2.resize(mask, self.resize_size, interpolation=cv2.INTER_NEAREST)\n",
    "    if self.resize_size:\n",
    "        image = cv2.resize(image, self.resize_size, interpolation=cv2.INTER_AREA)\n",
    "        mask = cv2.resize(mask, self.resize_size, interpolation=cv2.INTER_NEAREST)\n",
    "    if image.shape[0] > 1400 or image.shape[1] > 1400 :\n",
    "        ratio = ratioFunction(image.shape[0], image.shape[1])\n",
    "        new_size = (960, int(960*ratio))\n",
    "        image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n",
    "        mask = cv2.resize(mask, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "    if image.shape[0]%16 !=0 or image.shape[1]%16 !=0 :\n",
    "        new_size = (960, 720)\n",
    "        image = cv2.resize(image, new_size, interpolation=cv2.INTER_AREA)\n",
    "        mask = cv2.resize(mask, new_size, interpolation=cv2.INTER_NEAREST)\n",
    "    print(image.shape)\n",
    "    # apply data augmentations\n",
    "    if self.transform:\n",
    "      # ndarray to PIL image\n",
    "      image = Image.fromarray(image)\n",
    "      mask = Image.fromarray(mask)\n",
    "      # apply the transforms\n",
    "      image = self.transform(image)\n",
    "      mask = self.transform(mask)\n",
    "\n",
    "    #把RGB改成0,1,...,class_num : (w, w, 1)\n",
    "    mask = image2label(mask, self.cm2lbl)\n",
    "    mask = mask.squeeze()\n",
    "    # print(np.unique(mask))\n",
    "    # print(mask.shape)\n",
    "\n",
    "    # 把每個不同目標分成0,1 : (w, w, class_num)\n",
    "    masks = [(mask == v) for v in self.class_values]\n",
    "    mask = np.stack(masks, axis=-1).astype('float')\n",
    "\n",
    "    if self.transform_toTensor:\n",
    "      image = self.transform_toTensor(image)\n",
    "      mask = self.transform_toTensor(mask)\n",
    "\n",
    "    return image, mask\n",
    "    \n",
    "  def __len__(self):\n",
    "    return len(self.data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "880b6613",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_classes = ['guardrail']\n",
    "classes = ['guardrail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6283f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_vis_lanyard_re(weight_path, classes, test_features_folder, test_labels_folder, prefix=None):\n",
    "  best_model = torch.load(weight_path + '/best_model.pth')\n",
    "  test_input_imgs = read_images(test_features_folder)\n",
    "  test_output_imgs = read_images(test_labels_folder)\n",
    "\n",
    "  cm2lbl = make_cm2lbl()\n",
    "\n",
    "  test_dataset = IS_Dataset_test(\n",
    "      test_input_imgs, \n",
    "      test_output_imgs, \n",
    "#       transform=transform,\n",
    "      transform_toTensor=transform_toTensor,\n",
    "      classes=classes,\n",
    "      cm2lbl=cm2lbl\n",
    "   )\n",
    "  test_dataset_vis = IS_Dataset_test(test_input_imgs, test_output_imgs, classes=classes, cm2lbl=cm2lbl)\n",
    "\n",
    "  test_resized_dataset = IS_Dataset_test(\n",
    "      test_input_imgs, \n",
    "      test_output_imgs, \n",
    "#       transform=transform,\n",
    "      transform_toTensor=transform_toTensor,\n",
    "      classes=classes,\n",
    "      cm2lbl=cm2lbl,resize_size=(512,512)\n",
    "  )\n",
    "  \n",
    "  \n",
    "  for i in tqdm(range(20)):\n",
    "#     n = np.random.choice(len(test_dataset))\n",
    "    n = i\n",
    "    image_vis = test_dataset_vis[n][0].astype('uint8')\n",
    "\n",
    "    image, gt_mask = test_dataset[n]\n",
    "    image_resized, gt_mask_resized = test_resized_dataset[n]\n",
    "    \n",
    "    gt_mask = gt_mask.squeeze()\n",
    "#     gt_mask_tensor = torch.from_numpy(gt_mask)\n",
    "    gt_mask_tensor = gt_mask\n",
    "    \n",
    "    gt_mask_resized = gt_mask_resized.squeeze()\n",
    "    \n",
    "#     x_tensor = torch.from_numpy(image).to('cuda').unsqueeze(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    x_tensor = image.to('cuda').unsqueeze(0)\n",
    "    pr_mask = best_model(x_tensor).data\n",
    "    pr_mask = torch.sigmoid(pr_mask)\n",
    "#     print('pr_mask unique :', np.unique(pr_mask.squeeze().cpu().numpy()))\n",
    "    pr_mask = (pr_mask.squeeze().cpu().numpy().round())\n",
    "\n",
    "    \n",
    "#     x_tensor_resized = torch.from_numpy(image_resized).to('cuda').unsqueeze(0)\n",
    "    x_tensor_resized = image_resized.to('cuda').unsqueeze(0)\n",
    "    print('pr_mask shape:', pr_mask.shape)\n",
    "    pr_mask_resized = best_model(x_tensor_resized).data\n",
    "    pr_mask_resized = torch.sigmoid(pr_mask_resized)\n",
    "    pr_mask_resized = (pr_mask_resized.squeeze().cpu().numpy().round())\n",
    "    pr_mask_resized_return = cv2.resize(pr_mask_resized, (gt_mask.shape[1], gt_mask.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "#     print('pr_mask_resized_return shape:', pr_mask_resized_return.shape)\n",
    "    pr_mask_resized_return_uni = np.unique(pr_mask_resized_return)\n",
    "    pr_mask_resized_return_uni_list = pr_mask_resized_return_uni.tolist()\n",
    "    \n",
    "    \n",
    "\n",
    "    # save_images(save_img_dir, prefix_img, \n",
    "    #       image=image_vis, \n",
    "    #       ground_truth_mask=gt_mask, \n",
    "    #       predicted_mask=pr_mask,\n",
    "    #       prediced_resized_mask=pr_mask_resized_return,\n",
    "    #       mask_viz = lbl_viz\n",
    "    # )\n",
    "\n",
    "    visualize(\n",
    "        image=image_vis, \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted_mask=pr_mask,\n",
    "        prediced_resized_mask=pr_mask_resized_return,\n",
    "#         lbl_viz = lbl_viz\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9428eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path_Random = '/home/user/Desktop/lanyard_segmentation/prune/backups_prune/random/'\n",
    "weight_path_L1 = '/home/user/Desktop/lanyard_segmentation/prune/backups_prune/L1/'\n",
    "test_features_folder = '/home/user/Desktop/lanyard_segmentation/guardrail/test_data/features'\n",
    "test_labels_folder = '/home/user/Desktop/lanyard_segmentation/guardrail/test_data/labels'\n",
    "orig_features_folder = '/home/user/Desktop/lanyard_segmentation/guardrail/orig_features'\n",
    "orig_labels_folder = '/home/user/Desktop/lanyard_segmentation/guardrail/orig_labels'\n",
    "viz_classes = ['guardrail']\n",
    "classes = ['guardrail']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc18960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
